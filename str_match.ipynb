{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-14 18:06:54.156443: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-08-14 18:06:54.251090: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-08-14 18:06:56.563657: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "IMPORT LIBRARIES\n",
    "'''\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.callbacks import Callback\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, LSTM, Embedding, Bidirectional, Input, Concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "DATA PREPROCESSING\n",
    "1. Load data\n",
    "2. Divide data into train, validation, test with the first 70%, then 15%, then 15%\n",
    "3. Tokenize and pad the data\n",
    "'''\n",
    "filename = 'genomic_dataset_sr_train.txt'\n",
    "df = pd.read_csv(filename, sep='\\t', header=None, names=['1st_seq', '2nd_seq', 'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate data into train, validation, test\n",
    "train_data = df[:int(len(df)*0.7)]\n",
    "validate_data = df[int(len(df)*0.7):int(len(df)*0.85)]\n",
    "test_data = df[int(len(df)*0.85):]\n",
    "\n",
    "# divide data into different lists\n",
    "seq1_list = train_data['1st_seq'].tolist()\n",
    "seq2_list = train_data['2nd_seq'].tolist()\n",
    "label_list = train_data['label'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize and pad data\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(seq1_list + seq2_list)\n",
    "tokenized_seq1_list = tokenizer.texts_to_sequences(seq1_list)\n",
    "tokenized_seq2_list = tokenizer.texts_to_sequences(seq2_list)\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "max_len = max([len(seq) for seq in tokenized_seq1_list + tokenized_seq2_list])\n",
    "\n",
    "padded_seq1_list = pad_sequences(tokenized_seq1_list, maxlen=max_len, padding='post')\n",
    "padded_seq2_list = pad_sequences(tokenized_seq2_list, maxlen=max_len, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "MODEL TRAINING\n",
    "1. Add embedding layer\n",
    "2. Add LSTM layer\n",
    "3. Add Dense layer\n",
    "4. Compile model\n",
    "5. Fit model\n",
    "6. Save/Load model\n",
    "'''\n",
    "\n",
    "#input \n",
    "X = [padded_seq1_list, padded_seq2_list]\n",
    "# X = padded_seq1_list + padded_seq2_list\n",
    "y = label_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup dimensions\n",
    "input_dim = max_len\n",
    "output_dim = 1\n",
    "\n",
    "# setup model\n",
    "input1 = Input(shape=(None,))\n",
    "input2 = Input(shape=(None,))\n",
    "\n",
    "#embedding layer\n",
    "embed1 = Embedding(\n",
    "    input_dim=vocab_size,\n",
    "    output_dim=128,\n",
    ") (input1)\n",
    "embed2 = Embedding(\n",
    "    input_dim=vocab_size,\n",
    "    output_dim=128,\n",
    ") (input2)\n",
    "\n",
    "#concatenate layer\n",
    "concat_layer = Concatenate()([embed1, embed2])\n",
    "\n",
    "#bi-lstm layer\n",
    "bi_lstm_layer = Bidirectional(\n",
    "    LSTM(64, return_sequences=True)\n",
    ") (concat_layer)\n",
    "bi_lstm_layer = Bidirectional(\n",
    "    LSTM(64)\n",
    ") (bi_lstm_layer)\n",
    "output_layer = Dense(output_dim, activation='sigmoid') (bi_lstm_layer)\n",
    "\n",
    "model = Model(inputs=[input1, input2], outputs=output_layer)\n",
    "model.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup checkpoint\n",
    "class CustomModelCheckpoint(Callback):\n",
    "    def on_epoch_end(self, epoch: int, logs=None):\n",
    "        print('Epoch:', epoch)\n",
    "        print('Saving model...')\n",
    "        self.model.save('model.h5', overwrite=True)\n",
    "            \n",
    "custom_checkpoint = CustomModelCheckpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "CHOOSE EITHER ONE OF THE FOLLOWING:\n",
    "1. Load trained-model and continue training\n",
    "2. Train new model from scratch\n",
    "'''\n",
    "\n",
    "#! 1. Load trained-model\n",
    "\n",
    "#load model from file\n",
    "filename = 'model.h5'\n",
    "model = load_model(filename)\n",
    "epochs = 10\n",
    "batch_size = 32\n",
    "\n",
    "#fit model\n",
    "model.fit(X, y, epochs=epochs, batch_size=batch_size, callbacks=[custom_checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training info: \n",
      "Epoch 1/10\n",
      "    2/32813 [..............................] - ETA: 38:55:55 - loss: 0.6925 - accuracy: 0.5156"
     ]
    }
   ],
   "source": [
    "\n",
    "#! 2. TRAIN NEW MODEL\n",
    "\n",
    "print('Training info: ')\n",
    "# print(model.summary())\n",
    "\n",
    "epochs = 10\n",
    "batch_size = 32\n",
    "# model training\n",
    "model.fit(X, y, epochs=epochs, batch_size=batch_size, callbacks=[custom_checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-13 23:23:14.808087: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 166002944 exceeds 10% of free system memory.\n",
      "2023-08-13 23:23:14.867126: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 166002944 exceeds 10% of free system memory.\n",
      "2023-08-13 23:23:14.891813: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 166002944 exceeds 10% of free system memory.\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "MODEL PREDICT\n",
    "1. Load model\n",
    "2. Load data\n",
    "3. Predict\n",
    "4. Save result\n",
    "5. Analyze result\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
