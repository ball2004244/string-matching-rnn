{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-18 23:44:16.652623: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "IMPORT LIBRARIES\n",
    "'''\n",
    "import pandas as pd\n",
    "from typing import List, Tuple, Union\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model, load_model\n",
    "from keras.callbacks import Callback\n",
    "from keras.layers import Dense, LSTM, Embedding, Bidirectional, Input, Concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "DATA PREPROCESSING\n",
    "1. Load data\n",
    "2. Divide data into train, validation, test with the first 70%, then 15%, then 15%\n",
    "3. Tokenize and pad the data\n",
    "'''\n",
    "filename = 'genomic_dataset_sr_train.txt'\n",
    "df = pd.read_csv(filename, sep='\\t', header=None, names=['1st_seq', '2nd_seq', 'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate data into train, validation, test\n",
    "train_data = df[:int(len(df)*0.7)]\n",
    "validate_data = df[int(len(df)*0.7):int(len(df)*0.85)]\n",
    "test_data = df[int(len(df)*0.85):]\n",
    "\n",
    "# divide data into different lists\n",
    "seq1_list = train_data['1st_seq'].tolist()\n",
    "seq2_list = train_data['2nd_seq'].tolist()\n",
    "label_list = train_data['label'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize and pad data\n",
    "def data_preprocess(seq1_lst: List[str], seq2_lst: List[str]) -> Tuple[List[int], List[int], int, int]:\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(seq1_lst + seq2_lst)\n",
    "    tokenized_seq1_list = tokenizer.texts_to_sequences(seq1_lst)\n",
    "    tokenized_seq2_list = tokenizer.texts_to_sequences(seq2_lst)\n",
    "\n",
    "    vocab_size = len(tokenizer.word_index) + 1\n",
    "    max_len = max([len(seq) for seq in tokenized_seq1_list + tokenized_seq2_list])\n",
    "\n",
    "    padded_seq1_list = pad_sequences(tokenized_seq1_list, maxlen=max_len, padding='post')\n",
    "    padded_seq2_list = pad_sequences(tokenized_seq2_list, maxlen=max_len, padding='post')\n",
    "    \n",
    "    return padded_seq1_list, padded_seq2_list, vocab_size, max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_seq1_list, padded_seq2_list, vocab_size, max_len = data_preprocess(seq1_list, seq2_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "MODEL TRAINING\n",
    "1. Add embedding layer\n",
    "2. Add LSTM layer\n",
    "3. Add Dense layer\n",
    "4. Compile model\n",
    "5. Fit model\n",
    "6. Save/Load model\n",
    "'''\n",
    "\n",
    "#input \n",
    "X = [padded_seq1_list, padded_seq2_list]\n",
    "y = label_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup dimensions\n",
    "input_dim = max_len\n",
    "output_dim = 1\n",
    "\n",
    "# setup model\n",
    "input1 = Input(shape=(None,))\n",
    "input2 = Input(shape=(None,))\n",
    "\n",
    "#embedding layer\n",
    "embed1 = Embedding(\n",
    "    input_dim=vocab_size,\n",
    "    output_dim=128,\n",
    ") (input1)\n",
    "embed2 = Embedding(\n",
    "    input_dim=vocab_size,\n",
    "    output_dim=128,\n",
    ") (input2)\n",
    "\n",
    "#concatenate layer\n",
    "concat_layer = Concatenate()([embed1, embed2])\n",
    "\n",
    "#bi-lstm layer\n",
    "bi_lstm_layer = Bidirectional(\n",
    "    LSTM(64, return_sequences=True)\n",
    ") (concat_layer)\n",
    "bi_lstm_layer = Bidirectional(\n",
    "    LSTM(64)\n",
    ") (bi_lstm_layer)\n",
    "output_layer = Dense(output_dim, activation='softmax') (bi_lstm_layer)\n",
    "\n",
    "model = Model(inputs=[input1, input2], outputs=output_layer)\n",
    "model.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup checkpoint\n",
    "class CustomModelCheckpoint(Callback):\n",
    "    def on_epoch_end(self, epoch: int, logs=None):\n",
    "        print('Epoch:', epoch)\n",
    "        print('Saving model...')\n",
    "        self.model.save('model.keras', overwrite=True)\n",
    "            \n",
    "custom_checkpoint = CustomModelCheckpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "CHOOSE EITHER ONE OF THE FOLLOWING:\n",
    "1. Load trained-model and continue training\n",
    "2. Train new model from scratch\n",
    "'''\n",
    "\n",
    "#! 1. Load trained-model\n",
    "def load_trained_model(filename: str = 'model.keras'):\n",
    "    #load model from file\n",
    "    model = load_model(filename)\n",
    "    epochs = 10\n",
    "    batch_size = 32\n",
    "\n",
    "    # continue training\n",
    "    model.fit(X, y, epochs=epochs, batch_size=batch_size, callbacks=[custom_checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training info: \n",
      "Epoch 1/10\n",
      "    2/32813 [..............................] - ETA: 38:55:55 - loss: 0.6925 - accuracy: 0.5156"
     ]
    }
   ],
   "source": [
    "\n",
    "#! 2. TRAIN NEW MODEL\n",
    "def train_new_model():\n",
    "    epochs = 100\n",
    "    batch_size = 32\n",
    "    # model training\n",
    "    model.fit(X, y, epochs=epochs, batch_size=batch_size, callbacks=[custom_checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Start training...')\n",
    "train_new_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step\n",
      "[[0.9964434]]\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "[[0.9964434]]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "MODEL PREDICT\n",
    "1. Load model\n",
    "2. Load data\n",
    "3. Predict\n",
    "4. Save result\n",
    "5. Analyze result\n",
    "'''\n",
    "\n",
    "def predict(filename: str='model.h5') -> List[Union[bool, float]]:\n",
    "    model = load_model(filename)\n",
    "    \n",
    "    # sample_data = (\n",
    "    #     ['test_sequence_1'], \n",
    "    #     ['test_sequence_2']\n",
    "    # )\n",
    "    sample_data = (\n",
    "        ['test_sequence_1'],\n",
    "        ['another_sequence_10']\n",
    "    )\n",
    "    \n",
    "    processed_data = data_preprocess(sample_data[0], sample_data[1])\n",
    "    X_test = list(processed_data[:2])\n",
    "    \n",
    "    predictions = model.predict(X_test) # output as list of matching probability\n",
    "    \n",
    "    \n",
    "    threshold = 0.5\n",
    "    bool_predictions = [True if prediction > threshold else False for prediction in predictions]\n",
    "        \n",
    "    return predictions\n",
    "    # return bool_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Start predicting...')\n",
    "print(predict(filename='model.keras'))\n",
    "\n",
    "print('Done')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
